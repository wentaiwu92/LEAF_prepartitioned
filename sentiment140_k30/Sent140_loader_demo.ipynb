{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sent140_loader_demo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siZUw5niV_gr"
      },
      "source": [
        "### Sentiment140 data loading demo\n",
        "\n",
        "This is a data loading demo for the pre-partitioned sentiment140 (twitter) dataset. Through the following guide, we can load the whole data from files and create **one (torchtext) dataset for each (twitter) user**.\n",
        "\n",
        "The data is obtained by running the script from [LEAF](https://github.com/TalwalkarLab/leaf/tree/master/data/sent140). See more details in [my repo](https://github.com/wingter562/LEAF_prepartitioned/tree/main/sentiment140_k30)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTX8EgNiV6c9",
        "outputId": "4214692d-a51d-4a07-b9db-5b69b062b023"
      },
      "source": [
        "# check the directory tree and create a dir for the CIFAR dataset\n",
        "!ls -lh\n",
        "!mkdir datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 4.0K\n",
            "drwxr-xr-x 1 root root 4.0K Oct  8 13:45 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9wcKPSxW5IC"
      },
      "source": [
        "# import packages\n",
        "import torch\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_DO9AFbYcQ3"
      },
      "source": [
        "Download the pre-partitioned data, which has been split into two files for training and test beforehand.\n",
        "- training data file name: all_data_niid_0_keep_30_train_9.json\n",
        "- test file name: all_data_niid_0_keep_30_test_9.json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECEE0EOOX26w",
        "outputId": "5058106b-d301-459b-a9bd-8037935dbfd1"
      },
      "source": [
        "# download it from my repo\n",
        "%cd /content/datasets/\n",
        "!git clone https://github.com/wingter562/LEAF_prepartitioned.git\n",
        "%cd LEAF_prepartitioned/sentiment140_k30/\n",
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/datasets\n",
            "Cloning into 'LEAF_prepartitioned'...\n",
            "remote: Enumerating objects: 80, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 80 (delta 0), reused 12 (delta 0), pack-reused 56\u001b[K\n",
            "Unpacking objects: 100% (80/80), done.\n",
            "/content/datasets/LEAF_prepartitioned/sentiment140_k30\n",
            "total 25M\n",
            "-rw-r--r-- 1 root root 2.8M Oct 27 14:04 all_data_niid_0_keep_30_test_9.json\n",
            "-rw-r--r-- 1 root root  22M Oct 27 14:04 all_data_niid_0_keep_30_train_9.json\n",
            "-rw-r--r-- 1 root root 1.5K Oct 27 14:04 README.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNImSP23ZKS_"
      },
      "source": [
        "The data stored in the json files are in the following **format**: three key-value pairs at the top hierarchy, tweets data in the 3rd key-value pair as a dictionary.\n",
        "- \"users\": the list of users\n",
        "- \"num_samples\": the list of number of samples for each user\n",
        "- \"user_data\": \n",
        "  - username: \n",
        "    - \"x\": the list of the user's tweets, each tweet is a list containing the following items\n",
        "      - tweet id, e.g., \"1932561653\"\n",
        "      - date and timestamp, e.g., \"Tue May 26 21:43:20 PDT 2009\"\n",
        "      - query tag, e.g., \"NO_QUERY\"\n",
        "      - username (same as the key in the \"user_data\" dict), e.g., \"sunshine_diva\"\n",
        "      - tweet text, e.g., \"@faithgoddess7 I love the outdoors...but mostly in the Spring and Fall. Not too cold or too hot... \"\n",
        "      - train/test tag, e.g., \"training\"\n",
        "    - \"y\": the labels of the user's tweets as a list of binary values, e.g., \\[1 1 1 1 1 1 1 1 1 1\\].\n",
        "\n",
        "Based on the json data file structure, we define a function to extract the data, training and test separately, into two dictionaries using \"username\" as the keys.\n",
        "\n",
        "Note: tags like data/timestamp and query will be discarded as we are only interested in the tweet text and the label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwCdBpMTZJxA"
      },
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "import json\n",
        "\n",
        "def read_json(fpath):\n",
        "  assert fpath.endswith('.json')\n",
        "  udata = defaultdict(lambda : None)\n",
        "\n",
        "  print('loading %s...' % fpath)\n",
        "  with open(fpath, 'r') as f:\n",
        "    all_data = json.load(f)\n",
        "  users = all_data['users']\n",
        "  num_samples = all_data['num_samples']\n",
        "  tweets = all_data['user_data']\n",
        "  # parse user by user\n",
        "  for u in users:\n",
        "    # each user have multiple tweets, field 4 is the text for each tweet\n",
        "    u_tweets = [tw[4] for tw in tweets[u]['x']]\n",
        "    u_labels = tweets[u]['y']\n",
        "    udata[u] = {'x':u_tweets, 'y':u_labels}\n",
        "\n",
        "  #users = list(sorted(tweets.keys()))\n",
        "  return users, num_samples, udata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SbsMOBEiHBS"
      },
      "source": [
        "Next, we use the function to load the json files into dictionaries with \"username\" as the keys. The resulting train_data/test_data is in the following dictionary hierarchy:\n",
        "- username:\n",
        "  - 'x': the list of tweet texts\n",
        "  - 'y': the list of the corresponding labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAq7PiYEiFOa",
        "outputId": "d915c49b-db9e-4ed9-d8c7-eb3babe6a1a2"
      },
      "source": [
        "users, num_samples_train, train_data = read_json(\"all_data_niid_0_keep_30_train_9.json\")\n",
        "_, num_samples_test, test_data = read_json(\"all_data_niid_0_keep_30_test_9.json\")\n",
        "print(\"number of users: \", len(users))\n",
        "print(\"number of training samples: \", sum(num_samples_train))\n",
        "print(\"number of test samples: \", sum(num_samples_test))\n",
        "print(\"data sample format: \", train_data[users[0]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading all_data_niid_0_keep_30_train_9.json...\n",
            "loading all_data_niid_0_keep_30_test_9.json...\n",
            "number of users:  2875\n",
            "number of training samples:  130743\n",
            "number of test samples:  16025\n",
            "data sample format:  {'x': [\"@JennysMyName i'm sorry  p.s. Your mom can call tomorrow at 12:30pm or later. haah\", 'This is making me cry ', \"@DeeYoung08 i'm sorry  wish you could make it!\", '@Janettex3 i guess he did! It is rude! ', '@M4DDYL0V3SY0U lmao, wow! Poor Maddy ', '@GabrielaElena neither can i  and my knees give out :/', '@tishh i agree ', '@biancaalosa i know! Its so sad  its /soeasyy; just search email: uptownherolover@aim.com', 'Good morning twitter world. P.s. I miss them ', \"@RiskyBusinessMB i so wish i could've gone last night, i live 2 hours away   maybe another PPP with more notice?\", \"@biancaduhh awh! I'm sorry  my prayers are with your dad, &amp; his friends family &amp; friends.\", 'Last time in the drama room ', 'This is sad ', '@JennysMyName neither do i ', \"Freakin' cricket. Go away \", 'This black widow needs to go away!   i have a fear of spiders, and bleh.', '@Janettex3 i dont know  it wont let me upload! D:', 'missing the HS live chat today   someone please record it and put it on youtube. thanks!', \"@CathyDuhh 4pm PST. 7pm EST. and yes, thats why i'm missing it \", \"@ThePISTOL i hope i get through! I REALLY need encouragment cause i get sad  lol. Maybe i'll give you a rap preview (;\", '@desireeapril i will love, sorry you cant go ', 'going to clean, and then get ready. adios peeps. text/twitter me. whatevs you want. i miss last night ', '@JennysMyName grrr  i can try &amp; call in for us both.', '@Taylerose i want to but i dont have a printer that works at the moment ', '@DaisyDuhh yeah, i didt have a working printer so i couldnt ', '@desireeapril yes. I do. So still no contacts i dont think   gah. I dont feel pretty...EVER. Jeeze :/', '@JennysMyName im proud of you too Jenny! ', \"@JennysMyName baha, i'm gonna have to find some!  i dont think we took any D: but i'm gonna find some!\", '@yaykimo baaha  &amp; healthy choice my friend! (:', '@DeeYoung08 its gonna be great ', '@yaykimo yay! Glad you had fun  did you hug them all? (:', 'Pomona, here we come! ', '@M4DDYL0V3SY0U will do  haha.', '@M44DYL0V3Y0U wooot! Same with my &quot;nephew&quot; ', '@biancaalosa haha awh!  thats cute! He showed Cathy, Daisy, Stacy, &amp; Alejandra my Captain Alex gift (: it was cute. haha', \"@ThePISTOL correction. It was suppose to be like* the Cap'n Alex, haha  i said &quot;lik&quot;\", \"@Go4Valentine follow @honorsociety, they're AMAZING. I love them \", \"@JennysMyName bahaha  &quot;but Jenny, he's cute &amp; ye-&quot; - K &quot;NO! You have Mike, remember?!&quot;-J &quot;oh yah, sorry, i'm taken.&quot;-K\", \"@yayKIMO Kimo, yours is still better  bahaha. at least you didnt wear heels. but thanks to Joe, i'm scarred for life. lmao\", 'Girls are taking pictures of Jesse freaking out, lmao ', '@desireeapril yup! ', '@honorsociety i did yesterday!  8/2 &amp; 8/8!', '@JennysMyName hahah  can you tell me if that one video that cant be shown yet is showing up on my youtube? thanks.', \"@simoncurtis you're too cute (:  what i would GIVE to meet you&lt;3  have a nice day! \", 'JONAS cracks me up, lmao '], 'y': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hP4uemJSlv2O"
      },
      "source": [
        "In this demo, we wrap each twitter users' data into a **custom dataset** object so that each user's samples can be loaded and processed through the [new Pytorch NLP pipeline](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) for sentiment analysis.\n",
        "\n",
        "Native torchtext-style datasets (e.g., [torchtext.datasets.AG_NEWS](https://pytorch.org/text/stable/datasets.html)) are iterable (to enable dynamic loading of data streams) and yield a **(label, text)** tuple when iterated (see below). \n",
        "\n",
        "Nonetheless, in our case, a plain (map-style) dataset is enough for the pipeline.\n",
        "\n",
        "\n",
        "```\n",
        "# An example of native torchtext dataset\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "next(train_iter)\n",
        "\n",
        ">>> (3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green\n",
        "again.\")\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE66xU4ooNPk"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class Sent140_user_dataset(Dataset):\n",
        "  def __init__(self, username, user_data):\n",
        "    \"\"\"Create an IMDB dataset instance given a path and fields.\n",
        "    Arguments:\n",
        "    username: username\n",
        "    user_data: user data as a dictionary formatted as we describe above\n",
        "    \"\"\"\n",
        "    self.username = username\n",
        "    self.user_data = user_data\n",
        "    self.text = self.user_data['x']  # as a list of tweet text strings\n",
        "    self.labels = self.user_data['y']  # as a list of integers (either 0 or 1)\n",
        "    assert len(self.text) == len(self.labels)\n",
        "    self.size = len(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return tuple([self.labels[index], self.text[index]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaF6XngqGLxc"
      },
      "source": [
        "Now we wrap each the data, one dataset object for each user:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFA4fRfxGUdM",
        "outputId": "72c6ef7c-e8fb-485f-e37d-1f638266f44a"
      },
      "source": [
        "# one dataset object for one twitter user\n",
        "user_datasets = {}\n",
        "for u in users:\n",
        "  user_datasets[u] = Sent140_user_dataset(u, train_data[u])\n",
        "\n",
        "# pick one user and check it out\n",
        "some_user = 'KayleenDuhh'\n",
        "print(\"user:\", user_datasets[some_user].username)\n",
        "data_iterator = iter(user_datasets[some_user])\n",
        "print(\"first three tweets:\")\n",
        "print(next(data_iterator))\n",
        "print(next(data_iterator))\n",
        "print(next(data_iterator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user: KayleenDuhh\n",
            "first three tweets:\n",
            "(0, \"@JennysMyName i'm sorry  p.s. Your mom can call tomorrow at 12:30pm or later. haah\")\n",
            "(0, 'This is making me cry ')\n",
            "(0, \"@DeeYoung08 i'm sorry  wish you could make it!\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqRx3Fzli-Ku"
      },
      "source": [
        "Since each twitter user has very limited size of corpus, to **build a vocabulary** we need to use a global iterator (as a function) that iterates through all the tweets by all the users. The following codes build a global vocab collaboratively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5oljWIDhZSv",
        "outputId": "6ee8cf0b-89c9-4afd-d78a-d1bd315d1231"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(tokenizer='spacy', language='en_core_web_sm')  # the default is \"basic_english\"\n",
        "def yield_tokens(user_datasets):\n",
        "  for uname in user_datasets:\n",
        "    data_iter = iter(user_datasets[uname])\n",
        "    for _, text in data_iter:\n",
        "      yield tokenizer(text)\n",
        "\n",
        "tokens_iter = yield_tokens(user_datasets)\n",
        "print(next(tokens_iter))\n",
        "print(next(tokens_iter))\n",
        "\n",
        "# create vocab object, using torchtext apis and our token iterator function\n",
        "vocab = build_vocab_from_iterator(yield_tokens(user_datasets), specials=[\"<pad>\", \"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@JennysMyName', 'i', \"'m\", 'sorry', ' ', 'p.s', '.', 'Your', 'mom', 'can', 'call', 'tomorrow', 'at', '12:30pm', 'or', 'later', '.', 'haah']\n",
            "['This', 'is', 'making', 'me', 'cry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_YezoV3HiId"
      },
      "source": [
        "The vocab object maps words to their indices (or one-hot vectors equivalently) and also provides methods that reflect the information such as the size of the vocabulary and words frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21KOjaaFHfnI",
        "outputId": "40490563-86ba-445a-8795-5d5b298c4bfb"
      },
      "source": [
        "# test the vocab\n",
        "print(vocab(['here', 'is', 'an', 'example']))\n",
        "print(\"Vocabulary size: \", len(vocab))\n",
        "print(\"Special tokens and 8 most frequent tokens\", vocab.get_itos()[:10])\n",
        "print(\"Index of word 'winter':\", vocab['winter'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[97, 17, 124, 4486]\n",
            "Vocabulary size:  125979\n",
            "Special tokens and 8 most frequent tokens ['<pad>', '<unk>', '!', '.', 'I', ' ', ',', 'to', 'the', 'you']\n",
            "Index of word 'winter': 1797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO7w_-co1zwh"
      },
      "source": [
        "With a global vocab ready (logically visible to all the clients), we now can set up the dataloaders for local training. Here we demonstrate with the users' training sets.\n",
        "\n",
        "Loading text data is a bit tricky because sentences can vary in length. Depending on the model, a batch of sentences can be loaded in two ways:\n",
        "- The DataLoader **stacks** the sentences when producing a batch and thus requires them to be of same length. Here is where the \"padding\" operation comes in. \n",
        "- The DataLoader **concatenates** the sentences when producing a batch and records the offset of each sentence. This is to be coupled with \"**Bag-of-Words**\" models (see the book [McTear 2016](https://link.springer.com/content/pdf/10.1007/978-3-319-32967-3.pdf) and the [wiki link](https://en.wikipedia.org/wiki/Bag-of-words_model) for details) that typically use a special embedding layer (torch.nn.EmbeddingBag)\n",
        "\n",
        "We implement the **second** method by virtue of the pass-in function **collate_fn** demonstrated in this [Torchtext tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3EL9lAU2S83"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# define a sentence-to-indices pipeline using the tokenizer and vocab\n",
        "# e.g., text_pipe('winter is coming.') >>> [1797, 17, 355, 3]\n",
        "text_pipe = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "# the collate function is applied by the Pytorch Dataloader\n",
        "def collate_batch(batch):\n",
        "  label_list, text_list, offsets = [], [], [0]\n",
        "  for (label, text) in batch:\n",
        "    label_list.append(label)\n",
        "    processed_text = torch.tensor(text_pipe(text), dtype=torch.int)\n",
        "    text_list.append(processed_text)\n",
        "    offsets.append(len(processed_text))\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)  # cumulative offsets\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int)\n",
        "  text_list = torch.cat(text_list)  # concat them\n",
        "  return label_list, text_list, offsets\n",
        "\n",
        "# create dataloaders\n",
        "local_loaders = {}\n",
        "for u in users:\n",
        "  local_loaders[u] = DataLoader(user_datasets[u], \n",
        "                                batch_size=8, shuffle=False, \n",
        "                                collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx0aK7g63y-2"
      },
      "source": [
        "Now our local DataLoaders are ready. Check it out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oISDHfYKGD9n",
        "outputId": "dea06a5e-68a9-4676-b21e-b54b32f7cdfb"
      },
      "source": [
        "some_user = 'KayleenDuhh'\n",
        "print(\"The shape of the batches for '%s':\" % some_user)\n",
        "for labels, text, offsets in local_loaders[some_user]:\n",
        "  print(text.shape, \"with %d sentences\" % len(labels))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of the batches for 'KayleenDuhh':\n",
            "torch.Size([81]) with 8 sentences\n",
            "torch.Size([97]) with 8 sentences\n",
            "torch.Size([146]) with 8 sentences\n",
            "torch.Size([118]) with 8 sentences\n",
            "torch.Size([152]) with 8 sentences\n",
            "torch.Size([67]) with 5 sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQUq1dnhLHq1"
      },
      "source": [
        "Each batch generated by our DataLoader is a 1-D tensor because sentences got concatenated. This is the shape required by a special embedding layer called [torch.nn.EmbeddingBag](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag). The offsets are needed for model inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B81cw2bPK5Kf"
      },
      "source": [
        "Before we move on to define the embedding layer and our model, we may want to load **pre-trained embedding vectors** so that our model's embedding layer can be properly initialized.\n",
        "\n",
        "The vocab object we set up above is essentially a **word-to-index** dictionary. The new [torchtext Vocab](https://pytorch.org/text/stable/vocab.html) class no longer defines **index-to-vector** mapping (i.e., embedding), which means that we need to an extra object for pre-trained embeddings. The pre-training embeddings are [torchtext Vectors](https://pytorch.org/text/stable/vocab.html#pretrained-word-embeddings) objects.\n",
        "\n",
        "The following code loads **GloVe** as the pre-trained embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBHJd_EuXleX",
        "outputId": "d46225dd-16fa-4fc9-d743-45df3987e36f"
      },
      "source": [
        "import torchtext\n",
        "\n",
        "# 6 billion word, 100-dim version of GloVe\n",
        "glove_emb = torchtext.vocab.GloVe(name='6B', dim=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:41, 5.33MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:18<00:00, 22146.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAIoE3o3UqmW",
        "outputId": "aa4fae89-a36e-4bcb-d59a-f884501e1711"
      },
      "source": [
        "print(\"The embedding for word 'winter' from GloVe:\")\n",
        "print(glove_emb.get_vecs_by_tokens(\"winter\", lower_case_backup=True))\n",
        "print(\"The embedding shape for sentence '<pad> winter <unk>' from GloVe:\")\n",
        "print(glove_emb.get_vecs_by_tokens(['<pad>', 'winter', '<unk>'], lower_case_backup=True).shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding for word 'winter' from GloVe:\n",
            "tensor([-2.8663e-01,  6.7126e-01,  2.5696e-01, -3.4079e-01, -3.7587e-01,\n",
            "        -6.8104e-01, -6.2413e-02,  4.7955e-01, -1.2093e+00, -4.8105e-01,\n",
            "        -3.5496e-01, -7.1598e-01,  9.6647e-01, -1.1323e-01, -7.5996e-02,\n",
            "         2.0510e-02, -8.1094e-02, -1.6928e-01, -3.0567e-01, -2.7979e-01,\n",
            "        -3.6918e-01,  5.8724e-01,  8.2248e-01,  6.8072e-01, -4.6516e-01,\n",
            "         8.2520e-01, -7.3008e-01, -6.3272e-01,  9.8242e-02, -5.7056e-01,\n",
            "        -5.6975e-01, -4.0802e-01, -1.0243e-01, -5.2213e-01, -1.0497e-01,\n",
            "         8.2012e-01,  3.4849e-02,  2.7265e-01, -2.1298e-01, -1.6997e-01,\n",
            "        -1.1399e-01, -3.7558e-01, -9.0068e-02, -4.4219e-02,  9.6546e-01,\n",
            "         4.1123e-01,  5.0636e-01, -1.0452e+00,  1.0377e+00, -4.7861e-01,\n",
            "         2.0212e-01, -5.1381e-01,  3.6144e-01,  5.7141e-01, -1.2959e-01,\n",
            "        -2.5805e+00,  8.0368e-02,  5.4031e-01,  1.2210e+00,  2.3555e-01,\n",
            "        -2.9840e-01,  1.3073e+00, -2.7097e-01, -1.7336e-01,  1.0842e+00,\n",
            "        -2.3214e-01,  1.7561e-03, -1.2898e+00,  3.4606e-01, -6.6234e-01,\n",
            "        -1.2513e-01, -9.9820e-01, -7.1101e-01,  1.0501e+00,  5.2475e-02,\n",
            "         5.8317e-01,  2.1686e-01, -1.6920e-01, -5.3517e-01,  2.5379e-01,\n",
            "         8.4931e-02,  7.6466e-02, -7.2805e-01, -2.1709e-02, -2.2680e-01,\n",
            "         2.4296e-01, -2.0295e-01,  2.6008e-01,  5.2835e-01,  5.0138e-01,\n",
            "         6.0935e-01, -5.6449e-02,  5.3211e-01,  1.4845e-01, -6.3584e-01,\n",
            "         3.1699e-02, -5.3147e-01,  1.1790e+00,  3.0278e-01, -2.8893e-01])\n",
            "The embedding shape for sentence '<pad> winter <unk>' from GloVe:\n",
            "torch.Size([3, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQpKW1KOU9qd"
      },
      "source": [
        "Now we define our **Bag-of-Words model with a Feed-Forward Network (FFN)** as the classifier. The structure of our model (again, from the [TorchText tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html)) is shown below:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://pytorch.org/tutorials/_images/text_sentiment_ngrams_model.png\" width=\"450\" height=\"450\" />\n",
        "</p>\n",
        "<p align=\"center\">\n",
        "The BoW model from the <a href=\"https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\">TorchText tutorial</a> \n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAIhOZzIIBR0"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Model_BoW(nn.Module):\n",
        "  def __init__(self, embed_dim, out_dim, vocab, pretrained):\n",
        "    \"\"\"\n",
        "    embed_dim: number of dimensions for the embedding layer\n",
        "    out_dim: number of dimensions for the output layer\n",
        "    vocab: the vocabulary object\n",
        "    pretrained: the loaded pre-trained embeddings\n",
        "    \"\"\"\n",
        "    super(Model_BoW, self).__init__()\n",
        "    # self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "    self.embed_dim = embed_dim\n",
        "    self.out_dim = out_dim\n",
        "    assert self.embed_dim == pretrained.dim\n",
        "\n",
        "    # load pre-trained embedding as the initial weights\n",
        "    pretrained_weights = self.embeddings2weights_by_vocab(vocab, pretrained)\n",
        "    self.embedding = nn.EmbeddingBag.from_pretrained(pretrained_weights, sparse=False)\n",
        "\n",
        "    self.fc = nn.Linear(embed_dim, out_dim)\n",
        "    self.init_weights()\n",
        "  \n",
        "  def embeddings2weights_by_vocab(self, vocab, pretrained):\n",
        "    \"\"\"\n",
        "    Load the pre-trained embeddings by the order of vacabulary entries into weights.\n",
        "    Use uniform random for out-of-table tokens\n",
        "    \"\"\"\n",
        "    # vocab.get_itos() returns the list of tokens by index order\n",
        "    # glove_emb.get_vecs_by_tokens() can take a list of tokens and return a 2D tensor\n",
        "    return glove_emb.get_vecs_by_tokens(vocab.get_itos(), lower_case_backup=True)\n",
        "\n",
        "  def init_weights(self):\n",
        "    initrange = 0.5\n",
        "    #self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "    self.fc.bias.data.zero_()\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "    embedded = self.embedding(text, offsets)\n",
        "    out = self.fc(embedded)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0GiU8tOXN7"
      },
      "source": [
        "Now, using a global vocab and the loaded pre-trained embeddings, we can instantiate **a global model** that will be distributed to the clients to be trained on their local datasets/dataloaders we built earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g_3aWUFOr9g",
        "outputId": "c25dac99-a8c7-45e3-8cda-47210b68f53d"
      },
      "source": [
        "glob_model = Model_BoW(embed_dim=100, out_dim=1, vocab=vocab, pretrained=glove_emb)\n",
        "print(glob_model)\n",
        "\n",
        "# model inference\n",
        "some_user = 'KayleenDuhh'\n",
        "for batch in local_loaders[some_user]:\n",
        "  labels, text, offsets = batch\n",
        "  preds = glob_model(text, offsets).squeeze()\n",
        "  print(\"\\nModel predicts: \", preds.tolist())\n",
        "  print(\"Ground truth: \", labels.tolist())"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model_BoW(\n",
            "  (embedding): EmbeddingBag(125979, 100, mode=mean)\n",
            "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "Model predicts:  [0.3835672438144684, 0.3440566062927246, 0.34107184410095215, 0.36228814721107483, 0.49583756923675537, 0.3856084644794464, 0.5766748785972595, 0.5032239556312561]\n",
            "Ground truth:  [0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Model predicts:  [0.39959895610809326, 0.4715730845928192, 0.42841678857803345, 0.2638038694858551, 0.2840256989002228, 0.5685793161392212, 0.4764051139354706, 0.3809822201728821]\n",
            "Ground truth:  [0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Model predicts:  [0.5822653770446777, 0.42088374495506287, 0.47541746497154236, 0.35256025195121765, 0.381011962890625, 0.4372074604034424, 0.5040506720542908, 0.45315638184547424]\n",
            "Ground truth:  [0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Model predicts:  [0.45184630155563354, 0.4816446304321289, 0.42339685559272766, 0.5139278769493103, 0.4345231354236603, 0.41869232058525085, 0.4425433874130249, 0.5118927359580994]\n",
            "Ground truth:  [0, 0, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Model predicts:  [0.5031370520591736, 0.5070642232894897, 0.4647405743598938, 0.42510780692100525, 0.39723339676856995, 0.5074991583824158, 0.456727534532547, 0.4537999927997589]\n",
            "Ground truth:  [1, 1, 1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Model predicts:  [0.5020631551742554, 0.572364330291748, 0.4317089021205902, 0.4400451183319092, 0.434722900390625]\n",
            "Ground truth:  [1, 1, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvbNthAPWUVF"
      },
      "source": [
        "So far, user datasets are ready and the model has been defined for FL experiments. For the implementation of model training, evaluation and test, we refer the readers to the following links. They are local-logic based but can be transplanted to FL-logic easily.\n",
        "\n",
        "1. [TorchText tutorial](https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html) (new TorchText pipeline)\n",
        "2. [bentrevett pytorch-sentiment-analysis repo](https://github.com/bentrevett/pytorch-sentiment-analysis) (old TorchText pipeline)"
      ]
    }
  ]
}